{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "underlying-wells",
   "metadata": {},
   "source": [
    "## Short Study\n",
    "\n",
    "The purpose of this short study is to look at what has happened to GME over the last 2 years. \n",
    "\n",
    "## Data\n",
    "We are going to look at data from FINRA **RegSHO** available:  at http://regsho.finra.org/\n",
    "\n",
    "There are 5 sources of data: \n",
    "* CNMS:\n",
    "* FNQC:\n",
    "* FNRA:\n",
    "* FNSQ:\n",
    "* FNYX: \n",
    "\n",
    "The filenames are arranged as such:\n",
    "\n",
    "``` XXXXshvolYYYYMMDD.txt ```\n",
    "\n",
    "XXXX is one of the 5 data sources as an example\n",
    "\n",
    "```CNMSshvol20190102.txt```\n",
    "\n",
    "We need to generate a list of dates to pull from regSHO sites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "posted-blanket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['20190101', '20190102', '20190103', '20190104', '20190105', '20190106',\n",
       "       '20190107', '20190108', '20190109', '20190110',\n",
       "       ...\n",
       "       '20210723', '20210724', '20210725', '20210726', '20210727', '20210728',\n",
       "       '20210729', '20210730', '20210731', '20210801'],\n",
       "      dtype='object', length=944)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Generate dates to pull logs with\n",
    "x = pd.date_range(start='20190101',end='20210801',freq='D').strftime('%Y%m%d')\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-witch",
   "metadata": {},
   "source": [
    "You can see above that we can generate any range we want. I am going to store the data in my local drive under `shortdata/`\n",
    "\n",
    "As an example we will pull one day from regsho and write the data to a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "connected-pixel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "base_url = 'http://regsho.finra.org/'\n",
    "\n",
    "XXXX = 'CNMS'\n",
    "date = '20190102' # Jan 01 is a holiday and there is no data\n",
    "postfix = '.txt'\n",
    "filename = XXXX+'shvol'+date+postfix\n",
    "\n",
    "# Check to make sure we got it right\n",
    "ref_filename = 'CNMSshvol20190102.txt'\n",
    "print(filename==ref_filename)\n",
    "\n",
    "# point at where you want to store the data. \n",
    "dir = 'shortdata/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stainless-dining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://regsho.finra.org/CNMSshvol20190102.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function BufferedWriter.close>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will download the file using the requests library\n",
    "import requests\n",
    "\n",
    "url = base_url + filename\n",
    "print(url)\n",
    "r = requests.get(url)\n",
    "s = requests.get(url).content\n",
    "f=open(dir+filename,'wb')\n",
    "f.write(s)\n",
    "f.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "republican-clothing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 Error\n"
     ]
    }
   ],
   "source": [
    "# Now with a little error checking so we can skip weekends and holidays. \n",
    "\n",
    "date = '20190101' # Jan 01 is a holiday and there is no data\n",
    "filename = XXXX+'shvol'+date+postfix\n",
    "url = base_url + filename\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "if r.status_code == 404:\n",
    "    print('404 Error')\n",
    "else:\n",
    "    s = requests.get(url).content\n",
    "    f=open(dir+filename,'wb')\n",
    "    f.write(s)\n",
    "    f.close\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-score",
   "metadata": {},
   "source": [
    "## pull.py \n",
    "This is the function I use to get data from regsho \n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Generate dates to pull logs with\n",
    "x = pd.date_range(start='20190101',end='20210801',freq='D').strftime('%Y%m%d')\n",
    "\n",
    "\n",
    "burl = 'http://regsho.finra.org/'\n",
    "dir = 'shortdata/'\n",
    "#prefix = 'CNMSshvol' # NMS\n",
    "#prefix = 'FNQCshvol' # NMS\n",
    "#prefix = 'FNRAshvol' # NMS\n",
    "prefix = 'FNSQshvol' # NMS\n",
    "#prefix = 'FNYXshvol' # NMS\n",
    "\n",
    "postfix = '.txt'\n",
    "for idx in range(len(x)):\n",
    "    url =\"\"\n",
    "    pull = x[idx]\n",
    "    filename = prefix+pull+postfix\n",
    "    url =burl+filename\n",
    "    print(burl)\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if r.status_code == 404:\n",
    "        print('404 Error')\n",
    "    else:\n",
    "        s = requests.get(url).content\n",
    "        f=open(dir+filename,'wb')\n",
    "        f.write(s)\n",
    "        f.close\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-personal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
